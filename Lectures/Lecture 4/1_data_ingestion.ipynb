{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367af9ac-d756-44ef-8c7e-f19f53f578e9",
   "metadata": {},
   "source": [
    "# Ingesting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054245a2-0b6d-4758-91cf-ac1908b117dd",
   "metadata": {},
   "source": [
    "In python there are multiple ways to ingest data into a pandas dataframe.  This is because we often will work with data from various sources, even in a single project, and thus being able to easily ingest and format data is crucial to being productive.\n",
    "\n",
    "We will be looking at 6 different ways to ingest data today:\n",
    "- via csv\n",
    "- via JSON\n",
    "- via API\n",
    "- via google sheets\n",
    "- via SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118c773-301d-41ac-9bd9-23482e833449",
   "metadata": {},
   "source": [
    "## CSV ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f020a07-6354-47d8-986c-c0dddc8258c0",
   "metadata": {},
   "source": [
    "One of the most common data formats you'll deal with in the real world is csvs.  CSVs are an easy, text based representation of tabular data.  It can be easily saved (it's just a text file), passed around and while it's not the most efficient way to store data, it is less verbose than other formats like JSON. \n",
    "\n",
    "There are a few ways to read csv data into a pandas DataFrame.  The easiest way is to use `.read_csv`, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949fe94-9d7c-4287-bdff-2ad8e72c3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7b2e5-e907-417b-af8e-2c91f1084d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps = pd.read_csv('data/market_caps.csv')\n",
    "market_caps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09819c-e7cb-4023-bdb7-980d1c37b12f",
   "metadata": {},
   "source": [
    "We can also see that `.read_csv` will smartly try to format columns for us.  E.g. it can detect that MarketCap, Price and VolumeUSD are floats, and Rank is an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c77e13-ae42-41d0-9126-45b70f8bac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481a403-2446-4a1a-a7e0-6c688092c6a5",
   "metadata": {},
   "source": [
    "However, we can add a bit more customization to this.  For example, let's say we wanted custom names and to use rank as an index.  We can do so directly from `.read_csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b0f02-357e-4621-88d1-6e8a455cf515",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps_custom = pd.read_csv(\n",
    "    'data/market_caps.csv',\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    "    names=['rank', 'name', 'sym', 'market_size', 'price', 'volume_usd'],\n",
    "    index_col=0\n",
    ")\n",
    "market_caps_custom.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086eb99-065f-4929-92d9-bb39ec8b4d16",
   "metadata": {},
   "source": [
    "## JSON ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327c446-080b-4d91-bcc6-5c7661bcdcfd",
   "metadata": {},
   "source": [
    "JSONs have similar properties to csvs, in that they are simple text files that can be passed around easily, however they can be a bit harder to work with for data analysis.  \n",
    "\n",
    "While a JSON can represent a table, it can also represent any arbitrary bundle of data.  This means that we may need to parse the JSON first and extract out the relevant values before we can convert it into a pandas DataFrame.\n",
    "\n",
    "Let's start with a simple example where the JSON is nicely formatted, and represents a table.  In this case, like with `.read_csv`, ad can simply call `.read_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3990c9-221c-4b6e-9612-2d4bb43b3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e075c-b718-4165-a3b2-934ebbd6a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps_json = pd.read_json('data/market_caps.json', orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fae13f-6795-4b4a-96d9-492d716a58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80989fcf-91a6-4e12-afdb-ca50456f703b",
   "metadata": {},
   "source": [
    "However, this won't work with a more complex json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff246872-e023-48c6-bd47-68c9727a6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_json('data/payload.json', orient='columns')\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a796f85-7c5d-4ed1-a5d2-c7dd05a07717",
   "metadata": {},
   "source": [
    "This isn't what we're looking for.  What if we tried the other way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bcf77-7664-4332-b64e-c8f48c2bdcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_json('data/payload.json', orient='index')\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844aacb1-2751-4dda-bb01-4ea0661084e2",
   "metadata": {},
   "source": [
    "Unfortunately this doesn't help us either.  This is because in this case the JSON file is structured as a dictionary only, and not as a table.  Pandas will still try to read in the key/value pairs as a table and make things work, but in this case we won't be able to use it.\n",
    "\n",
    "Instead, we will need to first get down to the key we care about, then pass that to pandas, i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e1f3a-57a0-4669-969f-2954dd91011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/payload.json', 'r') as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34d41d-b8c8-43d3-9719-eaa2a9187171",
   "metadata": {},
   "outputs": [],
   "source": [
    "(lambda x: x.assign(ts=pd.to_datetime(x.ts, unit='s')))(\n",
    "    pd.DataFrame(\n",
    "        json_data['result']['3600'],\n",
    "        columns=['ts', 'open', 'high', 'low', 'close', 'volume', 'volumeUSD']\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea49063-ea68-4948-ab5a-2c51fd27a774",
   "metadata": {},
   "source": [
    "## API Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d10ca-55b4-44af-a04a-fca7d38b415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977d8ef-b60f-482b-9e58-521cfba972e8",
   "metadata": {},
   "source": [
    "we've mostly been using API for data retrieval in this course.  The majority of the time you'll be using the request package.  Here is what we have been using in class so far, but lets dig into what this means a little more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75753b1c-f5fe-42c4-9cf2-dd16384e6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\n",
    "    f'https://api.cryptowat.ch/markets/coinbase-pro/btcusd/ohlc',\n",
    "    params={\n",
    "        'periods': '3600',\n",
    "        'after': str(int(pd.Timestamp('2021-12-01').timestamp()))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9174f89e-f5eb-4d07-9f96-2f1a84224b98",
   "metadata": {},
   "source": [
    "most APIs that we will hit will be REST APIs (http based APIs). For these there are two types of calls that you can do, either `get` requests or `post` requests.  The main difference is that `get` requests will take in parameters directly in the URL, while `post` parameters will allow you to pass in data (usually called a payload) into request.\n",
    "\n",
    "For many apis you will also need to pass in an `Authorization` header if it needs authentication.\n",
    "\n",
    "Once we have called the API with our request, we will get a response.  Firstly, we can check if the response is successful by looking at the status code.  Usually REST APIs will denote success with a status code of `200`, and a status code of `400`, `500` or most other codes will mean failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9975f9-b4bf-4f7a-817b-a99a39dc6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd18570-0a5a-47ff-ad02-9fc63241cc98",
   "metadata": {},
   "source": [
    "We can see above that the status code is successful, so now we can take a look at the content of our response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8465ab-8985-447e-aa12-234df6e32194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f28774-fd18-471c-830c-6396d9e2629a",
   "metadata": {},
   "source": [
    "as you can see, all responses that is passed back from an API call can be read as text.  However in our case, we know ahead of time that the response body is a json, and so we can use `.json` method in the response object to convert the text into a python dict, which would be much easier for us to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f10fb-660f-41f3-91df-bf90bca0fa5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23141df-676d-4e00-8328-96e0c06b24c1",
   "metadata": {},
   "source": [
    "finally, we can use the same technique as the JSON section and pull the relevant information we need from the JSON and load it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e2d1f-b44e-4977-8b8d-42a39f569af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(lambda x: x.assign(ts=pd.to_datetime(x.ts, unit='s')))(\n",
    "    pd.DataFrame(\n",
    "        res.json()['result']['3600'],\n",
    "        columns=['ts', 'open', 'high', 'low', 'close', 'volume', 'volumeUSD']\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1718b-ad5f-4537-84a8-9a3a696d58ed",
   "metadata": {},
   "source": [
    "## Google sheets ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894e7ee-430d-4f72-b673-c9de54c13ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f088b9d-056f-4b46-889c-ef57cae78cc0",
   "metadata": {},
   "source": [
    "another useful source for ingestion is google sheets.  While you an ingest larger data sets, where google sheets really shines is for getting tabular information for summary data, or especially configs.\n",
    "\n",
    "A good example of this is running a model where certain parameters (e.g. priors for coefficients) are set by domain experts, while the model is developed by data scientists and will take these priors as an input to generate the final result.  In that situation the domain experts probably work much better in a spreadsheet setting, while the data scientists will prefer code.  We can leverage google sheets to allow the domain experts to enter and update the coefficients, then we can programatically read the values and join against the rest of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c08fd-868a-4730-a67a-0119a27181f6",
   "metadata": {},
   "source": [
    "Example usage for google sheets using the library gspread is below, and you can find the setup information here: https://github.com/burnash/gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741f9ce-cd8f-4fac-bc7f-6b99ac432bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = gspread.service_account()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d5b159-faea-464c-b169-929987e05dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet = gc.open('market_caps').worksheet('Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7f4e3-0706-42ec-8ae4-08501d3939a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sheet.get_all_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a76e80-f0cb-4929-9ce8-787bd25271cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e88d51-c0ca-4616-84e1-6648981bfd9f",
   "metadata": {},
   "source": [
    "## SQL Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9327d36-e0dc-426b-b8be-3f0ca3303b2a",
   "metadata": {},
   "source": [
    "one of the most common ways we will be accessing data is via SQL.  \n",
    "\n",
    "For our examples, we will look at how to access a sql database, read, write and join it using sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6376cc-e729-4db7-a95c-3ee1c4b744bf",
   "metadata": {},
   "source": [
    "## SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97475d6f-73fa-4baa-b5ae-ef5854af183b",
   "metadata": {},
   "source": [
    "SQLite is a powerful, file-based sql database.  It's build as a local database for single reader/writer functionality, and has most of the common SQL functionality out of the box.  We can learn about most python<>sql interactions with SQLite as the interaction patterns carries over to all flavors of sql.\n",
    "\n",
    "The two key objects for interacting with any SQL database are the `connection` and the `cursor`.  The connection represents a connection to a SQL database, while the cursor represents an execution object for that connection.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09adc543-2160-4fe3-8383-e654ecdd3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e8ea4-40f1-4b21-a2fc-b69808fe3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/data.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d2a272-e7e6-482c-ae85-b178d3f8d9c7",
   "metadata": {},
   "source": [
    "in the above, we create a connection to the SQL database at `data/data.db`.  The connection object can save changes with `.commit` or close the connection with `.close`.  We can also create a cursor object from the connection to use for querying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe60f34-20ee-4452-816c-4a04f0f32c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54bcf79-883f-413c-ac60-dd2d4c4e6baf",
   "metadata": {},
   "source": [
    "Now that we have the cursor object, we can start executing SQL queries.  For example, let's take a look at what tables we have in this database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5249b-c544-4e03-a9f5-4df74c6a2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in cur.execute('SELECT name FROM sqlite_master'):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687417a6-0c6f-4915-8636-69035c435de1",
   "metadata": {},
   "source": [
    "or even the schema itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40cb0f-48df-415d-88fa-cba64e0df9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in cur.execute('SELECT name, sql FROM sqlite_master'):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eea5ed-567c-44ba-bfa8-e91d523b10c1",
   "metadata": {},
   "source": [
    "Most of the time though, we want to pull information from the database.  For example, let's pull some data from the `ohlc` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127d218-1ef8-4c01-ad7d-9e22c5f649a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in cur.execute(\"SELECT * FROM ohlc WHERE token = 'BTC' LIMIT 5\"):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532859ee-d64a-4e97-90fc-a6920f813f6a",
   "metadata": {},
   "source": [
    "we can also also get all the data at once instead of getting data row by row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907bd492-f63d-4f9d-934c-5342f644b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT * FROM ohlc WHERE token = 'BTC' LIMIT 5\")\n",
    "data = cur.fetchall()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe9449-aae4-4559-aea2-066473073edf",
   "metadata": {},
   "source": [
    "And because it's a list of tuples, we can simply create a pandas DataFrame and add in columns to get our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39745e-de98-4aff-b8b2-7a1e9117fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data,\n",
    "    columns=['ts', 'open', 'high', 'low', 'close', 'volume', 'volumeUSD', 'token', 'chain']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707bc19d-703f-474f-8f4b-db264b159fa5",
   "metadata": {},
   "source": [
    "finally, we can close the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ea6a2-9081-45dd-b2d0-360e3cb6552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2282d2-2e11-4a59-bf48-5a4dc8d62046",
   "metadata": {},
   "source": [
    "after closing the connection, as expected the cursor will stop working, and no queries can be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dae78e-442b-4081-ae89-bc21aee0c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT * FROM ohlc WHERE token = 'BTC' LIMIT 5\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5939d97-dd58-47b9-8d19-4edbd103b482",
   "metadata": {},
   "source": [
    "## SQL Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa27ed-0802-4826-b704-e952345ea5ac",
   "metadata": {},
   "source": [
    "SQL is a very flexible language, and most data analysis tools (including pandas) borrows a lot of core concepts that SQL has when working with data.  In this section we'll look at a few basic SQL functions that will help us create the datasets that we need before we pull them to python side to work on them.\n",
    "\n",
    "Firstly, let's look at basic querying.  We can query for all columns using `*`, or select the specific columns.  The general syntax is `SELECT [columns | *] FROM [table_name] WHERE [conditions...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474c0a0-8a95-46e4-bda5-7ebeeedc69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql sqlite:///data/data.db\n",
    "\n",
    "SELECT * FROM ohlc WHERE token = 'BTC' LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08809c96-bf6d-416c-a736-c6fd14d7c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT ts, token, close FROM ohlc WHERE token = 'BTC' LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4425bc-0536-46d0-bbe3-1833f270abde",
   "metadata": {},
   "source": [
    "In addition, we can also aggregate very easily using the `GROUP BY` clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03d9a7-5175-47e7-a8e0-68ce360ee9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT \n",
    "    token, \n",
    "    SUM(volumeUSD) / 1e9 AS volumeUSD, \n",
    "    SUM(volume) AS volume, \n",
    "    AVG(close) AS avg_close \n",
    "FROM ohlc \n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2356797a-ee1a-4195-b777-299200fe387c",
   "metadata": {},
   "source": [
    "In addition, like with pandas we can do joins.  The notation is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e600719-7682-4203-8924-d56087d8bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "SELECT\n",
    "    ts,\n",
    "    token,\n",
    "    close,\n",
    "    MarketCap\n",
    "FROM\n",
    "    ohlc\n",
    "    JOIN market_caps ON ohlc.token = market_caps.Symbol\n",
    "LIMIT 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b48b0-c9c8-49f0-b2a4-3638734cf6c0",
   "metadata": {},
   "source": [
    "we can also get much fancier with our querying, using CTEs (common table expressions) and joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e9d87-a2a1-4521-b5f9-96b52e24d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "WITH\n",
    "\n",
    "volume_summaries AS (\n",
    "SELECT \n",
    "    token, \n",
    "    SUM(volumeUSD) / 1e9 AS volumeUSD\n",
    "FROM ohlc \n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC\n",
    "),\n",
    "\n",
    "token_caps AS (\n",
    "SELECT\n",
    "    Symbol AS token,\n",
    "    MarketCap / 1e9 AS market_cap\n",
    "FROM\n",
    "    market_caps\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    *,\n",
    "    ROUND(100 * volumeUSD / market_cap) AS float_traded_total\n",
    "FROM\n",
    "    token_caps\n",
    "    JOIN volume_summaries USING(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e4692-3106-4914-ae6c-bda43c0bb1c3",
   "metadata": {},
   "source": [
    "**note** for the magic function that we used `%%sql`, we can actually convert the data to a DataFrame instead of just displaying it on output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d885af-9648-400c-bf96-cd7585e00f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql result <<\n",
    "\n",
    "SELECT\n",
    "    ts,\n",
    "    token,\n",
    "    close,\n",
    "    MarketCap\n",
    "FROM\n",
    "    ohlc\n",
    "    JOIN market_caps ON ohlc.token = market_caps.Symbol\n",
    "LIMIT 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39adc46-3cc5-424f-8173-f7d23cc3aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.DataFrame().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841fa9f3-ebfa-4abd-80f8-cd0a5f5ff6a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SQL and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ac686-9c40-4c48-bb4b-6e38b72d93f1",
   "metadata": {},
   "source": [
    "We saw above how to use cursors to access the database, fetch data with SQL queries and convert the result to a dataframe.  However, we can do this just as easily with pandas directly using the `.read_sql` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43779edb-20c8-4cfc-8237-f77eede82947",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT * FROM ohlc WHERE token = 'BTC' LIMIT 10\", 'sqlite:///data/data.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da3aa5a-fab6-4d8c-9ecb-7a5be5f1795e",
   "metadata": {},
   "source": [
    "or if we don't want ot open/close the connection everytime, we can also create a connection first, then pass in the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7cbac-e328-4e90-812a-d3b904697a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/data.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ec716-0e49-41ea-954b-6b56071fae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT * FROM ohlc WHERE token = 'BTC' LIMIT 10\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c382d80-9c1c-4fdd-95bf-ab63958c3e27",
   "metadata": {},
   "source": [
    "Since the data coming back from sql will always be in a tabular form and will have type information for each column, this means that the data set will (almost) always convert nicely into a pandas dataframe.  In fact, we can also turn the schema lookup into a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130af9a5-441a-4abf-8423-52a51b6e6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pd.read_sql('SELECT name, sql FROM sqlite_master', conn)\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10765de-29b6-4618-b3ba-32f9979e826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.sql[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51784f92-b94c-41ee-8075-510786464226",
   "metadata": {},
   "source": [
    "We can actually use the queries above as is, and load it directly into a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4a87d-879e-4244-a6cd-0a1acb99311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = pd.read_sql('''\n",
    "WITH\n",
    "\n",
    "volume_summaries AS (\n",
    "SELECT \n",
    "    token, \n",
    "    SUM(volumeUSD) / 1e9 AS volumeUSD\n",
    "FROM ohlc \n",
    "GROUP BY 1 \n",
    "ORDER BY 2 DESC\n",
    "),\n",
    "\n",
    "token_caps AS (\n",
    "SELECT\n",
    "    Symbol AS token,\n",
    "    MarketCap / 1e9 AS market_cap\n",
    "FROM\n",
    "    market_caps\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    *,\n",
    "    ROUND(100 * volumeUSD / market_cap) AS float_traded_total\n",
    "FROM\n",
    "    token_caps\n",
    "    JOIN volume_summaries USING(token)\n",
    "''', conn)\n",
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2a794-2464-481f-b4a1-9b7d64c28e6f",
   "metadata": {},
   "source": [
    "We can also write back to our SQL database very easily, using `.to_sql`.  This allows us to process data in python, and once we're ready we can now save the results back down into a SQL database (which can be used for other people for example, or used by us later if we want to join against other datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ea946-f5ad-4fef-a4f1-38713cd19445",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries.to_sql('summaries', conn, if_exists='fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d163b84-685d-453d-bab7-b35562664527",
   "metadata": {},
   "source": [
    "notice that we set `if_exists` equal to `fail` (this is also the default option).  This prevents accidental overwrites, as if we try to run the command again it will detect the table already existing and fail.  If this is not the desired behavior we can also use `replace` or `append` options, which will either rewrite the table everytime, or append the data to the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5f95e-477c-477b-a124-e20ac067114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries.to_sql('summaries', conn, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24d491-c7d6-4cf2-911a-3bc8659567a3",
   "metadata": {},
   "source": [
    "lastly, if we want to drop any tables, we can just use a cursor and issue a drop command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67567995-3350-4b6a-8f78-86bfc2528ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.cursor().execute('DROP TABLE summaries')\n",
    "conn.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
